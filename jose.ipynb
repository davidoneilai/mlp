{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4928681f",
   "metadata": {},
   "source": [
    "# Implementação de MLP Multi-Camadas para Fashion MNIST\n",
    "\n",
    "Este notebook implementa uma rede neural do tipo Multilayer Perceptron (MLP) com múltiplas camadas ocultas usando apenas NumPy, sem frameworks de deep learning. Vamos usar o dataset Fashion MNIST para classificação de imagens de roupas e acessórios.\n",
    "\n",
    "## Objetivos\n",
    "1. Implementar uma MLP flexível com número arbitrário de camadas\n",
    "2. Comparar diferentes arquiteturas, ativações e inicializações\n",
    "3. Analisar os resultados para determinar a melhor configuração"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974c806d",
   "metadata": {},
   "source": [
    "# 1. Imports e Preparação dos Dados\n",
    "\n",
    "Vamos carregar o Fashion MNIST e preparar os dados para treinamento, validação e teste:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dbd8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Constantes\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Classes do Fashion MNIST\n",
    "class_names = [\n",
    "    \"Camiseta/Top\",\n",
    "    \"Calça\",\n",
    "    \"Pulôver\",\n",
    "    \"Vestido\",\n",
    "    \"Casaco\",\n",
    "    \"Sandália\",\n",
    "    \"Camisa\",\n",
    "    \"Tênis\",\n",
    "    \"Bolsa\",\n",
    "    \"Bota\",\n",
    "]\n",
    "\n",
    "# --- carregar dataset CSV do Kaggle ---\n",
    "try:\n",
    "    train_df = pd.read_csv(\"src/data/fashion_train.csv\")\n",
    "    test_df = pd.read_csv(\"src/data/fashion_test.csv\")\n",
    "    print(\n",
    "        f\"Dados carregados com sucesso: {train_df.shape[0]} amostras de treino, {test_df.shape[0]} de teste\"\n",
    "    )\n",
    "except FileNotFoundError:\n",
    "    print(\n",
    "        \"ERROR: Arquivos de dados não encontrados. Certifique-se de que os arquivos fashion_train.csv e fashion_test.csv estão na pasta src/data/\"\n",
    "    )\n",
    "\n",
    "# Normalização: dividir por 255 para obter valores entre 0 e 1\n",
    "X_train_full = train_df.iloc[:, 1:].values.astype(np.float32) / 255.0  #  60 000 × 784\n",
    "y_train_full = train_df.iloc[:, 0].values.reshape(-1, 1)  #  60 000 × 1\n",
    "X_test = test_df.iloc[:, 1:].values.astype(np.float32) / 255.0  #  10 000 × 784\n",
    "y_test = test_df.iloc[:, 0].values.reshape(-1, 1)  #  10 000 × 1\n",
    "\n",
    "# Converter rótulos para one-hot encoding\n",
    "enc = OneHotEncoder(sparse_output=False)\n",
    "y_train_full_oh = enc.fit_transform(y_train_full)\n",
    "y_test_oh = enc.transform(y_test)\n",
    "\n",
    "# Separar 10% dos dados de treino para validação\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full,\n",
    "    y_train_full_oh,\n",
    "    test_size=0.10,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y_train_full,\n",
    ")\n",
    "\n",
    "print(\"Dimensões dos dados:\")\n",
    "print(f\"  X_train: {X_train.shape}\")\n",
    "print(f\"  y_train: {y_train.shape}\")\n",
    "print(f\"  X_val: {X_val.shape}\")\n",
    "print(f\"  y_val: {y_val.shape}\")\n",
    "print(f\"  X_test: {X_test.shape}\")\n",
    "print(f\"  y_test: {y_test_oh.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b695b8",
   "metadata": {},
   "source": [
    "## Visualização de Exemplos do Dataset\n",
    "\n",
    "Vamos visualizar alguns exemplos para entender melhor os dados que estamos trabalhando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa988421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar alguns exemplos do dataset\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    img = X_train_full[i].reshape(28, 28)\n",
    "    plt.imshow(img, cmap=plt.cm.binary)\n",
    "    plt.xlabel(class_names[y_train_full[i][0]])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf793af",
   "metadata": {},
   "source": [
    "# 2. Implementação da MLP Multi-Camadas\n",
    "\n",
    "Vamos implementar uma MLP com suporte a múltiplas camadas, diferentes funções de ativação e métodos de inicialização de pesos.\n",
    "\n",
    "## Principais características:\n",
    "- Arquitetura flexível com número arbitrário de camadas ocultas\n",
    "- Suporte para diferentes funções de ativação (ReLU, Sigmoid, Tanh)\n",
    "- Inicialização de pesos otimizada (He, Xavier, Random)\n",
    "- Treino com mini-batch e momentum\n",
    "- Clipping de gradientes para evitar explosão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ab72be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.activation import Activation\n",
    "from src.utils.loss import Loss\n",
    "from src.utils.metrics import Metrics\n",
    "\n",
    "\n",
    "class MLPDeep:\n",
    "    \"\"\"Multilayer Perceptron com número arbitrário de camadas ocultas.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        layer_sizes: list[int],  # ex.: [784, 256, 128, 10]\n",
    "        activation: str = \"relu\",  # função p/ camadas ocultas\n",
    "        weight_init: str = \"he\",\n",
    "        learning_rate: float = 0.01,\n",
    "        momentum: float = 0.9,\n",
    "        use_bias: bool = True,\n",
    "        grad_clip: float = 1.0,  # valor para clipping de gradientes\n",
    "        random_state: int | None = None,\n",
    "    ):\n",
    "        \"\"\"Inicializa a rede neural.\n",
    "\n",
    "        Args:\n",
    "            layer_sizes: Lista com o tamanho de cada camada, incluindo entrada e saída\n",
    "            activation: Função de ativação para camadas ocultas ('relu', 'sigmoid', 'tanh')\n",
    "            weight_init: Método de inicialização de pesos ('he', 'xavier', 'random')\n",
    "            learning_rate: Taxa de aprendizado\n",
    "            momentum: Valor do momentum (0.0 = sem momentum)\n",
    "            use_bias: Se deve usar bias nas camadas\n",
    "            grad_clip: Valor máximo para clipping de gradientes\n",
    "            random_state: Semente para reprodutibilidade\n",
    "        \"\"\"\n",
    "        if random_state is not None:\n",
    "            np.random.seed(random_state)\n",
    "\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.L = len(layer_sizes) - 1  # nº de blocos W/b\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.use_bias = use_bias\n",
    "        self.grad_clip = grad_clip\n",
    "\n",
    "        self.act_name = activation.lower()\n",
    "        self.act, self.dact = Activation.get_activation_and_derivative(self.act_name)\n",
    "\n",
    "        # Inicializações\n",
    "        self._init_weights(weight_init)\n",
    "        self._init_velocity()\n",
    "\n",
    "        # Histórico de treinamento\n",
    "        self.history = {\n",
    "            \"train_loss\": [],\n",
    "            \"train_acc\": [],\n",
    "            \"val_loss\": [],\n",
    "            \"val_acc\": [],\n",
    "        }\n",
    "\n",
    "    # ---------- inicialização ----------\n",
    "    def _init_weights(self, method: str):\n",
    "        \"\"\"Inicializa os pesos da rede de acordo com o método especificado.\"\"\"\n",
    "        self.W, self.b = [], []\n",
    "        for i in range(self.L):\n",
    "            fan_in = self.layer_sizes[i]\n",
    "            fan_out = self.layer_sizes[i + 1]\n",
    "\n",
    "            # Escolha do método de inicialização\n",
    "            match method:\n",
    "                case \"xavier\":\n",
    "                    # Adequado para Sigmoid/Tanh\n",
    "                    std = math.sqrt(1 / fan_in)\n",
    "                case \"he\":\n",
    "                    # Adequado para ReLU\n",
    "                    std = math.sqrt(2 / fan_in)\n",
    "                case _:\n",
    "                    # Inicialização simples com valores pequenos\n",
    "                    std = 0.01\n",
    "\n",
    "            self.W.append(np.random.randn(fan_in, fan_out) * std)\n",
    "            self.b.append(np.zeros((1, fan_out)) if self.use_bias else None)\n",
    "\n",
    "    def _init_velocity(self):\n",
    "        \"\"\"Inicializa os termos de velocidade para o momentum.\"\"\"\n",
    "        self.vW = [np.zeros_like(w) for w in self.W]\n",
    "        self.vb = [np.zeros_like(b) if b is not None else None for b in self.b]\n",
    "\n",
    "    # ---------- forward pass ----------\n",
    "    def forward(self, X):\n",
    "        \"\"\"Realiza o forward pass pela rede neural.\n",
    "\n",
    "        Args:\n",
    "            X: Dados de entrada (batch_size, input_size)\n",
    "\n",
    "        Returns:\n",
    "            Saída da rede (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        self.z, self.a = [None] * self.L, [X]  # a[0] = input\n",
    "\n",
    "        # Propagação pelas camadas ocultas\n",
    "        for i in range(self.L - 1):\n",
    "            self.z[i] = self.a[i] @ self.W[i] + (self.b[i] if self.use_bias else 0)\n",
    "            # Clipping para evitar valores extremos\n",
    "            self.z[i] = np.clip(self.z[i], -500, 500)\n",
    "            self.a.append(self.act(self.z[i]))\n",
    "\n",
    "        # Última camada: sempre usa softmax\n",
    "        l = self.L - 1\n",
    "        self.z[l] = self.a[l] @ self.W[l] + (self.b[l] if self.use_bias else 0)\n",
    "        # Estabilização numérica da softmax é feita na implementação da função\n",
    "        self.a.append(Activation.softmax(self.z[l]))\n",
    "        return self.a[-1]\n",
    "\n",
    "    # ---------- backward pass ----------\n",
    "    def backward(self, X, y, y_hat):\n",
    "        \"\"\"Realiza o backward pass e atualiza os pesos.\n",
    "\n",
    "        Args:\n",
    "            X: Dados de entrada (batch_size, input_size)\n",
    "            y: Alvos (batch_size, output_size) em one-hot\n",
    "            y_hat: Saídas preditas (batch_size, output_size)\n",
    "        \"\"\"\n",
    "        batch_size = X.shape[0]\n",
    "        grads_W, grads_b = [None] * self.L, [None] * self.L\n",
    "\n",
    "        # 1. Camada de saída (erro de entropia cruzada com softmax)\n",
    "        delta = y_hat - y  # dL/dz_L (simplificação para CE+softmax)\n",
    "        l = self.L - 1\n",
    "        grads_W[l] = (\n",
    "            self.a[l].T @ delta\n",
    "        ) / batch_size  # Normalizar pelo tamanho do batch\n",
    "        grads_b[l] = delta.sum(axis=0, keepdims=True) / batch_size\n",
    "\n",
    "        # Clipping para evitar explosão de gradientes\n",
    "        grads_W[l] = np.clip(grads_W[l], -self.grad_clip, self.grad_clip)\n",
    "        grads_b[l] = np.clip(grads_b[l], -self.grad_clip, self.grad_clip)\n",
    "\n",
    "        # 2. Camadas ocultas (backpropagation)\n",
    "        for i in reversed(range(self.L - 1)):\n",
    "            delta = (delta @ self.W[i + 1].T) * self.dact(self.z[i])  # dL/dz_i\n",
    "            grads_W[i] = (self.a[i].T @ delta) / batch_size\n",
    "            grads_b[i] = delta.sum(axis=0, keepdims=True) / batch_size\n",
    "\n",
    "            # Clipping para todas as camadas\n",
    "            grads_W[i] = np.clip(grads_W[i], -self.grad_clip, self.grad_clip)\n",
    "            grads_b[i] = np.clip(grads_b[i], -self.grad_clip, self.grad_clip)\n",
    "\n",
    "        # 3. Atualização dos pesos com SGD + momentum\n",
    "        for i in range(self.L):\n",
    "            # Verificar NaN antes da atualização\n",
    "            if np.any(np.isnan(grads_W[i])) or np.any(\n",
    "                np.isnan(grads_b[i] if self.use_bias else 0)\n",
    "            ):\n",
    "                print(\n",
    "                    f\"Aviso: Gradientes NaN detectados na camada {i}. Pulando atualização.\"\n",
    "                )\n",
    "                continue\n",
    "\n",
    "            # Momentum: v_t = momentum * v_{t-1} + gradient\n",
    "            self.vW[i] = self.momentum * self.vW[i] + grads_W[i]\n",
    "            self.W[i] -= self.lr * self.vW[i]\n",
    "\n",
    "            if self.use_bias:\n",
    "                self.vb[i] = self.momentum * self.vb[i] + grads_b[i]\n",
    "                self.b[i] -= self.lr * self.vb[i]\n",
    "\n",
    "    # ---------- utilidades para treinamento ----------\n",
    "    def train_epoch(self, X, y, batch_size=128):\n",
    "        \"\"\"Treina o modelo por uma época completa.\n",
    "\n",
    "        Args:\n",
    "            X: Dados de treino\n",
    "            y: Alvos de treino (one-hot)\n",
    "            batch_size: Tamanho do mini-batch\n",
    "\n",
    "        Returns:\n",
    "            Perda média e acurácia média na época\n",
    "        \"\"\"\n",
    "        idx = np.random.permutation(len(X))  # Embaralhar dados\n",
    "        losses, accs = [], []\n",
    "\n",
    "        # Treinamento em mini-batches\n",
    "        for start in range(0, len(X), batch_size):\n",
    "            end = start + batch_size\n",
    "            Xb, yb = X[idx[start:end]], y[idx[start:end]]\n",
    "\n",
    "            # Forward pass\n",
    "            y_hat = self.forward(Xb)\n",
    "\n",
    "            # Backward pass e atualização\n",
    "            self.backward(Xb, yb, y_hat)\n",
    "\n",
    "            # Calcular métricas\n",
    "            loss = Loss.cross_entropy(yb, y_hat)\n",
    "            acc = (\n",
    "                Metrics.calculate_accuracy(yb, y_hat) * 100\n",
    "            )  # Converter para porcentagem\n",
    "\n",
    "            # Verificar valores inválidos\n",
    "            if not np.isnan(loss) and not np.isinf(loss):\n",
    "                losses.append(loss)\n",
    "                accs.append(acc)\n",
    "            else:\n",
    "                print(\"Aviso: Loss NaN/Inf detectada, pulando batch\")\n",
    "\n",
    "        return np.mean(losses), np.mean(accs)\n",
    "\n",
    "    def evaluate(self, X, y, batch_size=512):\n",
    "        \"\"\"Avalia o modelo em dados de validação/teste.\n",
    "\n",
    "        Args:\n",
    "            X: Dados de validação/teste\n",
    "            y: Alvos (one-hot)\n",
    "            batch_size: Tamanho do batch para avaliação\n",
    "\n",
    "        Returns:\n",
    "            Perda e acurácia\n",
    "        \"\"\"\n",
    "        self.eval_mode = True  # Modo de avaliação (não usado nesta implementação, mas útil para extensões)\n",
    "        preds = []\n",
    "\n",
    "        # Avaliação em batches para evitar estouro de memória\n",
    "        for start in range(0, len(X), batch_size):\n",
    "            end = start + batch_size\n",
    "            batch_preds = self.forward(X[start:end])\n",
    "            preds.append(batch_preds)\n",
    "\n",
    "        preds = np.vstack(preds)\n",
    "        loss = Loss.cross_entropy(y, preds)\n",
    "        acc = Metrics.calculate_accuracy(y, preds) * 100  # Converter para porcentagem\n",
    "\n",
    "        self.eval_mode = False\n",
    "        return loss, acc\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=30,\n",
    "        batch_size=128,\n",
    "        X_val=None,\n",
    "        y_val=None,\n",
    "        verbose=1,\n",
    "    ):\n",
    "        \"\"\"Treina o modelo por múltiplas épocas com log de progresso.\n",
    "\n",
    "        Args:\n",
    "            X_train: Dados de treino\n",
    "            y_train: Alvos de treino (one-hot)\n",
    "            epochs: Número de épocas\n",
    "            batch_size: Tamanho do mini-batch\n",
    "            X_val: Dados de validação (opcional)\n",
    "            y_val: Alvos de validação (opcional)\n",
    "            verbose: Nível de detalhamento (0=silencioso, 1=progresso, 2=detalhado)\n",
    "\n",
    "        Returns:\n",
    "            Histórico de treinamento\n",
    "        \"\"\"\n",
    "        best_val_acc = 0\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            # Treinar uma época\n",
    "            tr_loss, tr_acc = self.train_epoch(X_train, y_train, batch_size=batch_size)\n",
    "            self.history[\"train_loss\"].append(tr_loss)\n",
    "            self.history[\"train_acc\"].append(tr_acc)\n",
    "\n",
    "            # Validação\n",
    "            if X_val is not None and y_val is not None:\n",
    "                val_loss, val_acc = self.evaluate(X_val, y_val)\n",
    "                self.history[\"val_loss\"].append(val_loss)\n",
    "                self.history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "                # Log de progresso\n",
    "                if verbose > 0 and (epoch % 5 == 0 or epoch == epochs):\n",
    "                    print(\n",
    "                        f\"Época {epoch:3d}/{epochs}: \"\n",
    "                        f\"loss={tr_loss:.4f}, acc={tr_acc:.2f}%, \"\n",
    "                        f\"val_loss={val_loss:.4f}, val_acc={val_acc:.2f}%\"\n",
    "                    )\n",
    "\n",
    "                # Acompanhar melhor modelo (para early stopping)\n",
    "                if val_acc > best_val_acc:\n",
    "                    best_val_acc = val_acc\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "            elif verbose > 0 and (epoch % 5 == 0 or epoch == epochs):\n",
    "                print(\n",
    "                    f\"Época {epoch:3d}/{epochs}: loss={tr_loss:.4f}, acc={tr_acc:.2f}%\"\n",
    "                )\n",
    "\n",
    "        return self.history\n",
    "\n",
    "    def plot_history(self):\n",
    "        \"\"\"Plota o histórico de treinamento.\"\"\"\n",
    "        if not self.history[\"train_loss\"]:\n",
    "            print(\"Sem histórico de treinamento para mostrar.\")\n",
    "            return\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "        # Plot de loss\n",
    "        ax1.plot(self.history[\"train_loss\"], label=\"Treino\")\n",
    "        if self.history[\"val_loss\"]:\n",
    "            ax1.plot(self.history[\"val_loss\"], label=\"Validação\")\n",
    "        ax1.set_title(\"Loss durante treinamento\")\n",
    "        ax1.set_xlabel(\"Época\")\n",
    "        ax1.set_ylabel(\"Loss\")\n",
    "        ax1.legend()\n",
    "\n",
    "        # Plot de acurácia\n",
    "        ax2.plot(self.history[\"train_acc\"], label=\"Treino\")\n",
    "        if self.history[\"val_acc\"]:\n",
    "            ax2.plot(self.history[\"val_acc\"], label=\"Validação\")\n",
    "        ax2.set_title(\"Acurácia durante treinamento\")\n",
    "        ax2.set_xlabel(\"Época\")\n",
    "        ax2.set_ylabel(\"Acurácia (%)\")\n",
    "        ax2.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc0e917",
   "metadata": {},
   "source": [
    "# 3. Experimentos com Hiperparâmetros\n",
    "\n",
    "Vamos explorar diferentes configurações de hiperparâmetros para encontrar a melhor combinação. Os principais parâmetros a explorar são:\n",
    "\n",
    "1. **Arquitetura**: número e tamanho das camadas ocultas\n",
    "2. **Função de ativação**: ReLU, Sigmoid, Tanh\n",
    "3. **Inicialização de pesos**: He, Xavier, Random\n",
    "\n",
    "## 3.1 Definição da grade de hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50206953",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"hidden_arch\": [\n",
    "        [256],\n",
    "        [512],\n",
    "        [512, 256],\n",
    "        [256, 128],\n",
    "    ],  # Diferentes arquiteturas de camadas ocultas\n",
    "    \"act\": [\"relu\", \"tanh\", \"sigmoid\"],  # Funções de ativação\n",
    "    \"init\": [\"he\", \"xavier\", \"random\"],  # Métodos de inicialização\n",
    "}\n",
    "\n",
    "# Criar todas as combinações possíveis de hiperparâmetros\n",
    "experiments = list(\n",
    "    itertools.product(param_grid[\"hidden_arch\"], param_grid[\"act\"], param_grid[\"init\"])\n",
    ")\n",
    "\n",
    "print(f\"Total de {len(experiments)} combinações de hiperparâmetros para testar\")\n",
    "\n",
    "# Mostrar algumas combinações como exemplo\n",
    "print(\"\\nAlgumas combinações:\")\n",
    "for i, (hid, act, init) in enumerate(experiments[:5]):\n",
    "    print(f\"  {i + 1}. Camadas: {hid}, Ativação: {act}, Inicialização: {init}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9dddef",
   "metadata": {},
   "source": [
    "# 4. Execução dos Experimentos\n",
    "\n",
    "Vamos treinar modelos com todas as combinações de hiperparâmetros e armazenar os resultados.\n",
    "\n",
    "## 4.1 Função auxiliar para testes rápidos\n",
    "\n",
    "Primeiro, vamos testar se nossa implementação funciona corretamente com um único modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036c9290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar um único modelo para verificar se a implementação funciona\n",
    "def test_single_model():\n",
    "    model = MLPDeep(\n",
    "        layer_sizes=[784, 128, 10],\n",
    "        activation=\"relu\",\n",
    "        weight_init=\"he\",\n",
    "        learning_rate=0.01,\n",
    "        momentum=0.9,  # Adicionando momentum para ajudar na convergência\n",
    "        grad_clip=1.0,  # Clipping de gradientes\n",
    "        random_state=RANDOM_SEED,\n",
    "    )\n",
    "\n",
    "    # Treinar por algumas épocas com um conjunto menor de dados para teste rápido\n",
    "    X_sample = X_train[:5000]\n",
    "    y_sample = y_train[:5000]\n",
    "    X_val_sample = X_val[:1000]\n",
    "    y_val_sample = y_val[:1000]\n",
    "\n",
    "    model.fit(\n",
    "        X_sample,\n",
    "        y_sample,\n",
    "        epochs=3,\n",
    "        batch_size=128,\n",
    "        X_val=X_val_sample,\n",
    "        y_val=y_val_sample,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Plotar resultados\n",
    "    model.plot_history()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Executar o teste\n",
    "test_model = test_single_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ecaf37",
   "metadata": {},
   "source": [
    "## 4.2 Laço principal de experimentos\n",
    "\n",
    "Agora vamos executar todos os experimentos e salvar os resultados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f956e6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros de treinamento\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9  # Usar momentum para melhorar convergência\n",
    "\n",
    "results = []\n",
    "\n",
    "# Para experimentos mais rápidos, podemos usar um subconjunto dos dados\n",
    "# Remova os comentários abaixo para usar todos os dados\n",
    "X_train_exp = X_train\n",
    "y_train_exp = y_train\n",
    "X_val_exp = X_val\n",
    "y_val_exp = y_val\n",
    "\n",
    "# Para teste rápido, use um subconjunto dos dados\n",
    "# X_train_exp = X_train[:10000]  # Usando apenas 10.000 exemplos para treino rápido\n",
    "# y_train_exp = y_train[:10000]\n",
    "# X_val_exp = X_val\n",
    "# y_val_exp = y_val\n",
    "\n",
    "print(f\"Iniciando {len(experiments)} experimentos com {EPOCHS} épocas cada...\")\n",
    "\n",
    "for i, (hid, act, init) in enumerate(experiments):\n",
    "    print(f\"\\n=== Experimento {i + 1}/{len(experiments)}: {hid} | {act} | {init} ===\")\n",
    "\n",
    "    # Criar e treinar o modelo\n",
    "    model = MLPDeep(\n",
    "        layer_sizes=[784, *hid, 10],\n",
    "        activation=act,\n",
    "        weight_init=init,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        momentum=MOMENTUM,\n",
    "        grad_clip=1.0,  # Valor de clipping para evitar explosão\n",
    "        random_state=RANDOM_SEED,\n",
    "    )\n",
    "\n",
    "    # Treinamento\n",
    "    history = model.fit(\n",
    "        X_train_exp,\n",
    "        y_train_exp,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        X_val=X_val_exp,\n",
    "        y_val=y_val_exp,\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    # Avaliar no conjunto de validação completo\n",
    "    val_loss, val_acc = model.evaluate(X_val, y_val)\n",
    "\n",
    "    # Salvar resultados\n",
    "    results.append(\n",
    "        {\n",
    "            \"hidden\": str(hid),\n",
    "            \"activation\": act,\n",
    "            \"init\": init,\n",
    "            \"acc\": val_acc,\n",
    "            \"loss\": val_loss,\n",
    "            \"model\": model,  # Guardar o modelo para uso posterior\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"Resultado: loss={val_loss:.4f}, acc={val_acc:.2f}%\")\n",
    "\n",
    "print(\"\\nTodos os experimentos foram concluídos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98248333",
   "metadata": {},
   "source": [
    "# 5. Análise dos Resultados\n",
    "\n",
    "Vamos analisar os resultados dos experimentos para identificar a melhor configuração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b593e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criamos um DataFrame sem o modelo para exibição\n",
    "results_df = [{k: v for k, v in r.items() if k != \"model\"} for r in results]\n",
    "df = pd.DataFrame(results_df).sort_values(\"acc\", ascending=False)\n",
    "\n",
    "# Exibir os melhores resultados\n",
    "print(\"Top 5 melhores configurações:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6b34f4",
   "metadata": {},
   "source": [
    "## 5.1 Análise do melhor modelo\n",
    "\n",
    "Vamos analisar mais detalhadamente o melhor modelo encontrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dd65e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter o melhor modelo\n",
    "best_idx = df.index[0]\n",
    "best_model = results[best_idx][\"model\"]\n",
    "best_config = df.iloc[0]\n",
    "\n",
    "print(\"Melhor configuração encontrada:\")\n",
    "print(f\"  Arquitetura: {best_config['hidden']}\")\n",
    "print(f\"  Ativação: {best_config['activation']}\")\n",
    "print(f\"  Inicialização: {best_config['init']}\")\n",
    "print(f\"  Acurácia de validação: {best_config['val_acc']:.2f}%\")\n",
    "print(f\"  Loss de validação: {best_config['val_loss']:.4f}\")\n",
    "\n",
    "# Plotar histórico de treinamento do melhor modelo\n",
    "best_model.plot_history()\n",
    "\n",
    "# Avaliar no conjunto de teste\n",
    "test_loss, test_acc = best_model.evaluate(X_test, y_test_oh)\n",
    "print(\"\\nDesempenho no conjunto de teste:\")\n",
    "print(f\"  Loss: {test_loss:.4f}\")\n",
    "print(f\"  Acurácia: {test_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ea9fe4",
   "metadata": {},
   "source": [
    "## 5.2 Matriz de Confusão\n",
    "\n",
    "Vamos visualizar a matriz de confusão para entender melhor os erros do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabf6736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar previsões no conjunto de teste\n",
    "y_pred = best_model.forward(X_test)\n",
    "y_pred_class = np.argmax(y_pred, axis=1)\n",
    "y_true_class = np.argmax(y_test_oh, axis=1)\n",
    "\n",
    "# Calcular matriz de confusão\n",
    "cm = confusion_matrix(y_true_class, y_pred_class)\n",
    "\n",
    "# Plotar matriz de confusão\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    ")\n",
    "plt.xlabel(\"Predição\")\n",
    "plt.ylabel(\"Valor Real\")\n",
    "plt.title(\"Matriz de Confusão\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f2ba74",
   "metadata": {},
   "source": [
    "# 6. Conclusões\n",
    "\n",
    "Neste notebook, implementamos uma rede neural MLP de múltiplas camadas do zero usando apenas NumPy. Principais pontos:\n",
    "\n",
    "1. **Arquitetura**: As arquiteturas mais profundas ([256, 128]) tenderam a ter melhor desempenho que as mais simples ([128]).\n",
    "\n",
    "2. **Função de Ativação**: ReLU teve o melhor desempenho geral para este experimento, superando Sigmoid e Tanh.\n",
    "\n",
    "3. **Inicialização de Pesos**: As inicializações He e Xavier funcionaram melhor que a inicialização aleatória simples."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
