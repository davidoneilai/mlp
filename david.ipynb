{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "529964ab",
   "metadata": {},
   "source": [
    "# Comparativo: Mini-Batches vs Batch Completo com e sem Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa08343",
   "metadata": {},
   "source": [
    "### üìä Gr√°ficos Comparativos\n",
    "\n",
    "![Batch vs Full - Sem Clipping](batch_vs_full_sem_clipping.png)\n",
    "![Batch vs Full - Com Clipping](batch_vs_full_com_clipping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435c2fa4",
   "metadata": {},
   "source": [
    "### üîπ Matrizes de Confus√£o\n",
    "\n",
    "**Batch Completo - Sem Clipping**\n",
    "\n",
    "![Confusion Matrix - Sem Clipping](confusion_matrix_batch_completo_sem_clipping.png)\n",
    "\n",
    "**Batch Completo - Com Clipping**\n",
    "\n",
    "![Confusion Matrix - Com Clipping](confusion_matrix_batch_completo_com_clipping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b172823a",
   "metadata": {},
   "source": [
    "### üß† Por que utilizamos Gradient Clipping quando eu quis testar o batch total?\n",
    "\n",
    "Durante os testes com treinamento usando batch completo (60.000 amostras), observamos que o modelo apresentava explos√£o de gradientes, evidenciada por valores extremamente altos de *loss* (acima de 30) e uma acur√°cia de 10% (equivalente a chute aleat√≥rio).\n",
    "\n",
    "Esse problema ocorre porque o uso de batches grandes reduz a frequ√™ncia de atualiza√ß√£o dos pesos, concentrando muito gradiente em uma √∫nica atualiza√ß√£o, o que pode provocar valores extremos nas ativa√ß√µes e pesos. Isso afeta fun√ß√µes como softmax e causa instabilidade num√©rica.\n",
    "\n",
    "Para resolver isso, implementamos o **gradient clipping**, que limita a norma L2 dos gradientes a um valor m√°ximo (neste caso, 5.0). Se a norma dos gradientes for maior que esse limite, todos os gradientes s√£o escalados proporcionalmente. Isso estabiliza o treinamento, evita explos√µes e melhora a converg√™ncia.\n",
    "\n",
    "Com a t√©cnica aplicada, conseguimos treinar a rede mesmo com batch completo, mantendo a *loss* dentro de valores razo√°veis e observando uma melhoria na acur√°cia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80bce22",
   "metadata": {},
   "source": [
    "### üî¢ Como o Gradient Clipping funciona matematicamente?\n",
    "\n",
    "Seja $\\mathbf{g} \\in \\mathbb{R}^n$ o vetor concatenado de todos os gradientes do modelo (por exemplo, pesos e bias). O clipping por norma L2 funciona da seguinte forma:\n",
    "\n",
    "1. Calcula-se a norma:\n",
    "\n",
    "   $$\n",
    "   \\|\\mathbf{g}\\|_2 = \\sqrt{g_1^2 + g_2^2 + \\ldots + g_n^2}\n",
    "   $$\n",
    "\n",
    "2. Se $\\|\\mathbf{g}\\|_2 \\leq \\tau$, nada √© feito ($\\tau$ √© o limite definido, como 5.0).\n",
    "\n",
    "3. Caso contr√°rio, aplica-se um fator de escala:\n",
    "\n",
    "   $$\n",
    "   \\mathbf{g}_{\\text{clipped}} = \\mathbf{g} \\cdot \\frac{\\tau}{\\|\\mathbf{g}\\|_2 + \\varepsilon}\n",
    "   $$\n",
    "\n",
    "Esse reescalonamento garante que o vetor de gradiente final tenha no m√°ximo a norma desejada, sem alterar sua dire√ß√£o.\n",
    "\n",
    "Isso evita que gradientes extremamente grandes causem saltos muito bruscos na atualiza√ß√£o dos pesos, estabilizando o aprendizado, especialmente em redes profundas ou com batches grandes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a627cc",
   "metadata": {},
   "source": [
    "### ‚ùå Sem usar o Clipping:\n",
    "```\n",
    "    Epoch 1/30, Loss: 2.4086, Acc: 0.1053, Val Loss: 27.9799, Val Acc: 0.1899\n",
    "    Epoch 2/30, Loss: 27.9862, Acc: 0.1897, Val Loss: 31.0849, Val Acc: 0.1000\n",
    "    Epoch 3/30, Loss: 31.0849, Acc: 0.1000, Val Loss: 28.6563, Val Acc: 0.1701\n",
    "    Epoch 4/30, Loss: 28.5377, Acc: 0.1735, Val Loss: 28.3900, Val Acc: 0.1004\n",
    "    Epoch 5/30, Loss: 28.3917, Acc: 0.1003, Val Loss: 31.0711, Val Acc: 0.1004\n",
    "    Epoch 6/30, Loss: 31.0734, Acc: 0.1003, Val Loss: 24.2246, Val Acc: 0.1004\n",
    "    Epoch 7/30, Loss: 24.2253, Acc: 0.1003, Val Loss: 31.0711, Val Acc: 0.1004\n",
    "    Epoch 8/30, Loss: 31.0740, Acc: 0.1003, Val Loss: 31.0849, Val Acc: 0.1000\n",
    "    Epoch 9/30, Loss: 31.0826, Acc: 0.1001, Val Loss: 27.8344, Val Acc: 0.1004\n",
    "    Epoch 10/30, Loss: 27.8383, Acc: 0.1003, Val Loss: 31.0711, Val Acc: 0.1004\n",
    "    Epoch 11/30, Loss: 31.0751, Acc: 0.1003, Val Loss: 31.0711, Val Acc: 0.1004\n",
    "    Epoch 12/30, Loss: 31.0740, Acc: 0.1003, Val Loss: 30.2935, Val Acc: 0.1004\n",
    "    Epoch 13/30, Loss: 30.2958, Acc: 0.1003, Val Loss: 31.0711, Val Acc: 0.1004\n",
    "    Epoch 14/30, Loss: 31.0728, Acc: 0.1003, Val Loss: 25.2368, Val Acc: 0.1004\n",
    "    Epoch 15/30, Loss: 25.2385, Acc: 0.1003, Val Loss: 31.0711, Val Acc: 0.1004\n",
    "    Epoch 16/30, Loss: 31.0728, Acc: 0.1003, Val Loss: 26.7380, Val Acc: 0.1000\n",
    "    Epoch 17/30, Loss: 26.7355, Acc: 0.1001, Val Loss: 27.0227, Val Acc: 0.1004\n",
    "    Epoch 18/30, Loss: 27.0250, Acc: 0.1003, Val Loss: 28.9454, Val Acc: 0.1004\n",
    "    Epoch 19/30, Loss: 28.9477, Acc: 0.1003, Val Loss: 31.0711, Val Acc: 0.1004\n",
    "    Epoch 20/30, Loss: 31.0734, Acc: 0.1003, Val Loss: 31.0711, Val Acc: 0.1004\n",
    "    Epoch 21/30, Loss: 31.0745, Acc: 0.1003, Val Loss: 31.0711, Val Acc: 0.1004\n",
    "    Epoch 22/30, Loss: 31.0734, Acc: 0.1003, Val Loss: 27.4549, Val Acc: 0.1004\n",
    "    Epoch 23/30, Loss: 27.4566, Acc: 0.1003, Val Loss: 29.5507, Val Acc: 0.1004\n",
    "    Epoch 24/30, Loss: 29.5524, Acc: 0.1003, Val Loss: 31.0711, Val Acc: 0.1004\n",
    "    Epoch 25/30, Loss: 31.0728, Acc: 0.1003, Val Loss: 31.0711, Val Acc: 0.1004\n",
    "    Epoch 26/30, Loss: 31.0728, Acc: 0.1003, Val Loss: 26.2962, Val Acc: 0.1000\n",
    "    Epoch 27/30, Loss: 26.2939, Acc: 0.1001, Val Loss: 26.9832, Val Acc: 0.1004\n",
    "    Epoch 28/30, Loss: 26.9852, Acc: 0.1003, Val Loss: 27.8167, Val Acc: 0.1004\n",
    "    Epoch 29/30, Loss: 27.8189, Acc: 0.1003, Val Loss: 31.0711, Val Acc: 0.1004\n",
    "    Epoch 30/30, Loss: 31.0734, Acc: 0.1003, Val Loss: 31.0711, Val Acc: 0.1004\n",
    "    Batch Completo - Test Accuracy: 0.1004 | Test Loss: 31.0711\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a92c7e",
   "metadata": {},
   "source": [
    "### ‚úÖ Depois do Clipping:\n",
    "```\n",
    "    Epoch 1/30, Loss: 2.5320, Acc: 0.1170, Val Loss: 2.3967, Val Acc: 0.1661\n",
    "    Epoch 2/30, Loss: 2.3971, Acc: 0.1635, Val Loss: 2.2847, Val Acc: 0.2195\n",
    "    Epoch 3/30, Loss: 2.2860, Acc: 0.2144, Val Loss: 2.1911, Val Acc: 0.2662\n",
    "    Epoch 4/30, Loss: 2.1933, Acc: 0.2648, Val Loss: 2.1113, Val Acc: 0.3188\n",
    "    Epoch 5/30, Loss: 2.1142, Acc: 0.3202, Val Loss: 2.0400, Val Acc: 0.3723\n",
    "    Epoch 6/30, Loss: 2.0432, Acc: 0.3690, Val Loss: 1.9731, Val Acc: 0.4137\n",
    "    Epoch 7/30, Loss: 1.9765, Acc: 0.4097, Val Loss: 1.9090, Val Acc: 0.4507\n",
    "    Epoch 8/30, Loss: 1.9124, Acc: 0.4494, Val Loss: 1.8474, Val Acc: 0.4855\n",
    "    Epoch 9/30, Loss: 1.8508, Acc: 0.4842, Val Loss: 1.7882, Val Acc: 0.5178\n",
    "    Epoch 10/30, Loss: 1.7916, Acc: 0.5136, Val Loss: 1.7316, Val Acc: 0.5396\n",
    "    Epoch 11/30, Loss: 1.7349, Acc: 0.5374, Val Loss: 1.6774, Val Acc: 0.5585\n",
    "    Epoch 12/30, Loss: 1.6807, Acc: 0.5587, Val Loss: 1.6256, Val Acc: 0.5766\n",
    "    Epoch 13/30, Loss: 1.6289, Acc: 0.5769, Val Loss: 1.5762, Val Acc: 0.5899\n",
    "    Epoch 14/30, Loss: 1.5792, Acc: 0.5916, Val Loss: 1.5289, Val Acc: 0.6014\n",
    "    Epoch 15/30, Loss: 1.5317, Acc: 0.6036, Val Loss: 1.4836, Val Acc: 0.6120\n",
    "    Epoch 16/30, Loss: 1.4863, Acc: 0.6133, Val Loss: 1.4403, Val Acc: 0.6191\n",
    "    Epoch 17/30, Loss: 1.4428, Acc: 0.6213, Val Loss: 1.3990, Val Acc: 0.6263\n",
    "    Epoch 18/30, Loss: 1.4012, Acc: 0.6287, Val Loss: 1.3594, Val Acc: 0.6340\n",
    "    Epoch 19/30, Loss: 1.3615, Acc: 0.6344, Val Loss: 1.3217, Val Acc: 0.6394\n",
    "    Epoch 20/30, Loss: 1.3236, Acc: 0.6397, Val Loss: 1.2856, Val Acc: 0.6450\n",
    "    Epoch 21/30, Loss: 1.2873, Acc: 0.6445, Val Loss: 1.2512, Val Acc: 0.6495\n",
    "    Epoch 22/30, Loss: 1.2527, Acc: 0.6487, Val Loss: 1.2185, Val Acc: 0.6534\n",
    "    Epoch 23/30, Loss: 1.2197, Acc: 0.6530, Val Loss: 1.1873, Val Acc: 0.6584\n",
    "    Epoch 24/30, Loss: 1.1883, Acc: 0.6573, Val Loss: 1.1575, Val Acc: 0.6625\n",
    "    Epoch 25/30, Loss: 1.1583, Acc: 0.6619, Val Loss: 1.1293, Val Acc: 0.6676\n",
    "    Epoch 26/30, Loss: 1.1298, Acc: 0.6655, Val Loss: 1.1024, Val Acc: 0.6706\n",
    "    Epoch 27/30, Loss: 1.1027, Acc: 0.6695, Val Loss: 1.0768, Val Acc: 0.6753\n",
    "    Epoch 28/30, Loss: 1.0769, Acc: 0.6742, Val Loss: 1.0525, Val Acc: 0.6792\n",
    "    Epoch 29/30, Loss: 1.0524, Acc: 0.6790, Val Loss: 1.0294, Val Acc: 0.6831\n",
    "    Epoch 30/30, Loss: 1.0291, Acc: 0.6828, Val Loss: 1.0075, Val Acc: 0.6882\n",
    "    Batch Completo - Test Accuracy: 0.6882 | Test Loss: 1.0075\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8468ed1",
   "metadata": {},
   "source": [
    "Essa compara√ß√£o evidencia o impacto direto do gradient clipping na estabilidade e performance de treinamento quando se usa batch completo."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
