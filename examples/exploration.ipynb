{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5728fb8",
   "metadata": {},
   "source": [
    "# Exploração Inicial do Dataset Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5808aa",
   "metadata": {},
   "source": [
    "Neste notebook, vamos explorar o dataset Fashion MNIST e testar nossa implementação de MLP. Primeiro vamos carregar as bibliotecas necessárias e os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8678e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from example import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0dfc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os dados\n",
    "train = pd.read_csv('data/fashion-mnist_train.csv')\n",
    "test = pd.read_csv('data/fashion-mnist_test.csv')\n",
    "\n",
    "print(f\"Formato do conjunto de treinamento: {train.shape}\")\n",
    "print(f\"Formato do conjunto de teste: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9ba98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da666cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribuição das classes\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='label', data=train)\n",
    "plt.title('Distribuição das Classes no Conjunto de Treinamento')\n",
    "plt.xlabel('Classe')\n",
    "plt.ylabel('Contagem')\n",
    "\n",
    "# Mapear rótulos para nomes das classes\n",
    "class_names = ['Camiseta', 'Calça', 'Pulôver', 'Vestido', 'Casaco', \n",
    "              'Sandália', 'Camisa', 'Tênis', 'Bolsa', 'Bota']\n",
    "plt.xticks(range(10), class_names, rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd020956",
   "metadata": {},
   "source": [
    "## Visualização de Exemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655646ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_examples(data, labels, num_examples=5):\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i in range(num_examples):\n",
    "        plt.subplot(1, num_examples, i+1)\n",
    "        # Converter vetor para matriz 28x28\n",
    "        img = data.iloc[i, 1:].values.reshape(28, 28)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(class_names[labels.iloc[i]])\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Mostrar 5 exemplos aleatórios\n",
    "random_indices = np.random.randint(0, len(train), 5)\n",
    "plot_examples(train.iloc[random_indices], train.iloc[random_indices]['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8464f9",
   "metadata": {},
   "source": [
    "## Preparação dos Dados\n",
    "\n",
    "Vamos preparar os dados para treinamento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd48e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separar features e labels\n",
    "X_train = train.drop('label', axis=1).values / 255.0  # Normalizar para [0, 1]\n",
    "y_train = train['label'].values\n",
    "\n",
    "X_test = test.drop('label', axis=1).values / 255.0\n",
    "y_test = test['label'].values\n",
    "\n",
    "# Converter para one-hot encoding\n",
    "def to_onehot(y, num_classes=10):\n",
    "    onehot = np.zeros((len(y), num_classes))\n",
    "    onehot[np.arange(len(y)), y] = 1\n",
    "    return onehot\n",
    "\n",
    "y_train_onehot = to_onehot(y_train)\n",
    "y_test_onehot = to_onehot(y_test)\n",
    "\n",
    "# Separar um conjunto de validação\n",
    "X_train, X_val, y_train_onehot, y_val_onehot = train_test_split(\n",
    "    X_train, y_train_onehot, test_size=0.1, random_state=42)\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train_onehot.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561b728b",
   "metadata": {},
   "source": [
    "## Treinamento da MLP\n",
    "\n",
    "Agora vamos treinar um modelo MLP usando nossa implementação personalizada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b93b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando uma instância da MLP\n",
    "mlp = MLP(\n",
    "    input_size=784,          # 28x28 pixels\n",
    "    hidden_size=128,         # Número de neurônios na camada oculta\n",
    "    output_size=10,          # 10 classes\n",
    "    activation='sigmoid',    # Função de ativação\n",
    "    learning_rate=0.01,      # Taxa de aprendizado\n",
    "    momentum=0.9,            # Termo de momentum\n",
    "    weight_init='xavier',    # Inicialização de pesos\n",
    "    random_state=42          # Para reprodutibilidade\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc274594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o modelo\n",
    "history = mlp.train(\n",
    "    X_train,\n",
    "    y_train_onehot,\n",
    "    epochs=10,                  # Número de épocas\n",
    "    batch_size=64,              # Tamanho do mini-batch\n",
    "    validation_data=(X_val, y_val_onehot),\n",
    "    verbose=True,\n",
    "    log_interval=1              # Mostrar progresso a cada época\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1d1442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar curvas de aprendizado\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Perda\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['loss'], label='Treinamento')\n",
    "plt.plot(history['val_loss'], label='Validação')\n",
    "plt.title('Curva de Perda')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Perda')\n",
    "plt.legend()\n",
    "\n",
    "# Acurácia\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['accuracy'], label='Treinamento')\n",
    "plt.plot(history['val_accuracy'], label='Validação')\n",
    "plt.title('Curva de Acurácia')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Acurácia')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5adebab5",
   "metadata": {},
   "source": [
    "## Avaliação no Conjunto de Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff73478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliar no conjunto de teste\n",
    "test_loss, test_acc = mlp.evaluate(X_test, y_test_onehot)\n",
    "print(f\"Perda no teste: {test_loss:.4f}\")\n",
    "print(f\"Acurácia no teste: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e056787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predições no conjunto de teste\n",
    "y_pred = mlp.predict_classes(X_test)\n",
    "\n",
    "# Matriz de confusão\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title('Matriz de Confusão')\n",
    "plt.xlabel('Predição')\n",
    "plt.ylabel('Verdadeiro')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4be1e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relatório de classificação\n",
    "print(\"Relatório de Classificação:\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2441ad1f",
   "metadata": {},
   "source": [
    "## Salvando e Carregando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c887790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar o modelo\n",
    "model_path = 'fashion_mnist_model.pkl'\n",
    "mlp.save(model_path)\n",
    "print(f\"Modelo salvo em {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccf44d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o modelo\n",
    "loaded_mlp = MLP.load(model_path)\n",
    "print(\"Modelo carregado com sucesso!\")\n",
    "\n",
    "# Verificar se o modelo carregado tem o mesmo desempenho\n",
    "loaded_loss, loaded_acc = loaded_mlp.evaluate(X_test, y_test_onehot)\n",
    "print(f\"Perda no teste (modelo carregado): {loaded_loss:.4f}\")\n",
    "print(f\"Acurácia no teste (modelo carregado): {loaded_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cabe23",
   "metadata": {},
   "source": [
    "## Visualização de Exemplos Preditos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b035303f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar algumas predições\n",
    "def plot_predictions(X, y_true, y_pred, num_examples=5):\n",
    "    indices = np.random.choice(len(X), num_examples, replace=False)\n",
    "\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i, idx in enumerate(indices):\n",
    "        plt.subplot(1, num_examples, i+1)\n",
    "        img = X[idx].reshape(28, 28)\n",
    "        plt.imshow(img, cmap='gray')\n",
    "\n",
    "        pred_class = class_names[y_pred[idx]]\n",
    "        true_class = class_names[y_true[idx]]\n",
    "\n",
    "        color = 'green' if pred_class == true_class else 'red'\n",
    "        plt.title(f\"Pred: {pred_class}\\nTrue: {true_class}\", color=color)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Mostrar exemplos com suas predições\n",
    "plot_predictions(X_test, y_test, y_pred, num_examples=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00f24c5",
   "metadata": {},
   "source": [
    "## Experimentos com Hiperparâmetros\n",
    "\n",
    "Podemos testar diferentes configurações para otimizar o desempenho:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39211832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para treinar e avaliar um modelo com configurações específicas\n",
    "def experiment(hidden_size, activation, learning_rate, momentum, weight_init, epochs=5):\n",
    "    print(f\"\\nExperimento: hidden_size={hidden_size}, activation={activation}, \"\n",
    "          f\"lr={learning_rate}, momentum={momentum}, init={weight_init}\")\n",
    "\n",
    "    model = MLP(\n",
    "        input_size=784,\n",
    "        hidden_size=hidden_size,\n",
    "        output_size=10,\n",
    "        activation=activation,\n",
    "        learning_rate=learning_rate,\n",
    "        momentum=momentum,\n",
    "        weight_init=weight_init,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Treinar com um subset para economizar tempo\n",
    "    subset_size = 10000\n",
    "    X_subset = X_train[:subset_size]\n",
    "    y_subset = y_train_onehot[:subset_size]\n",
    "\n",
    "    history = model.train(\n",
    "        X_subset,\n",
    "        y_subset,\n",
    "        epochs=epochs,\n",
    "        batch_size=64,\n",
    "        validation_data=(X_val, y_val_onehot),\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test_onehot)\n",
    "    print(f\"Acurácia no teste: {test_acc:.4f}\")\n",
    "\n",
    "    return model, history, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf06810c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimento 1: ReLU com inicialização He\n",
    "model1, history1, acc1 = experiment(\n",
    "    hidden_size=128, \n",
    "    activation='relu', \n",
    "    learning_rate=0.001, \n",
    "    momentum=0.9, \n",
    "    weight_init='he'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e4d579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimento 2: Tanh com inicialização Xavier\n",
    "model2, history2, acc2 = experiment(\n",
    "    hidden_size=128, \n",
    "    activation='tanh', \n",
    "    learning_rate=0.001, \n",
    "    momentum=0.9, \n",
    "    weight_init='xavier'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d12fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimento 3: Sigmoid com taxa de aprendizado maior\n",
    "model3, history3, acc3 = experiment(\n",
    "    hidden_size=128, \n",
    "    activation='sigmoid', \n",
    "    learning_rate=0.01, \n",
    "    momentum=0.9, \n",
    "    weight_init='xavier'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ceca5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar resultados\n",
    "results = {\n",
    "    'ReLU + He': acc1,\n",
    "    'Tanh + Xavier': acc2,\n",
    "    'Sigmoid + Xavier': acc3\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(results.keys(), results.values())\n",
    "plt.ylim(0.7, 1.0)  # Ajustar para melhor visualização\n",
    "plt.title('Comparação de Acurácia por Configuração')\n",
    "plt.ylabel('Acurácia no Teste')\n",
    "for i, (k, v) in enumerate(results.items()):\n",
    "    plt.text(i, v + 0.01, f'{v:.4f}', ha='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39815c3",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Neste notebook, exploramos o dataset Fashion MNIST e implementamos uma MLP com diversos parâmetros configuráveis. Os principais pontos observados foram:\n",
    "\n",
    "1. A inicialização de pesos adequada para cada função de ativação é crucial\n",
    "2. O uso de momentum melhora a convergência do treinamento\n",
    "3. Diferentes funções de ativação têm desempenhos distintos neste problema\n",
    "\n",
    "Para melhorar ainda mais o desempenho, poderíamos:\n",
    "- Adicionar mais camadas ocultas\n",
    "- Implementar técnicas como dropout e batch normalization\n",
    "- Explorar outras técnicas de otimização como Adam, RMSProp\n",
    "- Realizar uma busca mais ampla de hiperparâmetros"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
